# -*- coding: utf-8 -*-
"""paniit_team_OneTeam.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tJdpfFbutYBmsmbWv0z8svKa0q_7QFpy
"""

# Keras
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation
from keras.layers.embeddings import Embedding
## Plotly
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
# Others
import nltk
import string
import numpy as np
import pandas as pd
from nltk.corpus import stopwords
from sklearn.manifold import TSNE


import numpy as np
np.random.seed(0)
from keras.models import Model
from keras.layers import Dense, Input, Dropout, LSTM, Activation
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
from keras.initializers import glorot_uniform
np.random.seed(1)

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('drive/My Drive/glove.6B.200d.txt')

def read_glove_vecs(glove_file):
    with open(glove_file, 'r') as f:
        words = set()
        word_to_vec_map = {}
        for line in f:
            line = line.strip().split()
            curr_word = line[0]
            words.add(curr_word)
            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)
        
        i = 1
        words_to_index = {}
        index_to_words = {}
        for w in sorted(words):
            words_to_index[w] = i
            index_to_words[i] = w
            i = i + 1
    return words_to_index, index_to_words, word_to_vec_map

### CHecking the working of word_to_index and index_to_word
word = "postgraduate"
index = 228777
print("the index of", word, "in the vocabulary is", word_to_index[word])
print("the", str(index) + "th word in the vocabulary is", index_to_word[index])

### Finally not used
def sentence_to_avg(sentence, word_to_vec_map):
  
    words = sentence.lower().split()
    avg = np.zeros(np.size(word_to_vec_map[words[0]]))
    for w in words:
        avg += word_to_vec_map[w]
    avg = avg/len(words)
    return avg

### Checking above piece of code
avg = sentence_to_avg("UG", word_to_vec_map)
print("avg = ", avg)
len(avg)

### returns the sentences to indices conversion
def sentences_to_indices(X, word_to_index, max_len):
    
    m = X.shape[0]                                   # number of training examples
    X_indices = np.zeros((m,max_len))
    
    for i in range(m):                               # loop over training examples
        sentence_words =X[i].lower().split()
        j = 0
        for w in sentence_words:
            X_indices[i, j] = word_to_index[w]
            j = j+1
            
    
    return X_indices

#### Checking sentences_to_indices function
X1 = np.array(["funny lol", "lets play baseball", "food is ready for you"])
X1_indices = sentences_to_indices(X1,word_to_index, max_len = 30)
print("X1 =", X1)
print("X1_indices =", X1_indices)

### Embedding layer output is returned
def pretrained_embedding_layer(word_to_vec_map, word_to_index):
    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)
    emb_dim = word_to_vec_map["cucumber"].shape[0]     
    emb_matrix = np.zeros((vocab_len,emb_dim))
    for word, index in word_to_index.items():
        emb_matrix[index, :] = word_to_vec_map[word]
    embedding_layer = Embedding(input_dim = vocab_len, output_dim = emb_dim,trainable = False)
    embedding_layer.build((None,))
    embedding_layer.set_weights([emb_matrix])
    
    return embedding_layer

from keras.layers import Bidirectional, Sequential
def classify(input_shape, word_to_vec_map, word_to_index):
    sentence_indices = Input(input_shape, dtype='int32')
    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)
    embeddings = embedding_layer(sentence_indices)   
    X = LSTM(32, return_sequences=True)(embeddings)
    #X = Dropout(0.5)(X)
    X = LSTM(32, return_sequences=False)(X)
    #X = Dropout(0.5)(X)
    X = Dense(10)(X)
    X = Activation('softmax')(X)

    model = Model(inputs=sentence_indices, outputs=X)
    return model

model = classify((10,), word_to_vec_map, word_to_index)
model.summary()

def convert_to_one_hot(Y, C):
    Y = np.eye(C)[Y.reshape(-1)]
    return Y

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

import pandas
import re
from operator import itemgetter
df = pandas.read_csv('drive/My Drive/naukri_com-job_sample.csv')
Y = {}
Y_num = {}
top_Y = {}
X = []
pred = []
ind = 0
# print(df['education'][0].lower()) 

for i in range(21999) :
  y = df['industry'][i]
  if(type(y) == str) :
    if y not in Y :
      Y[y] = ind
      Y_num[y] = 1
      ind += 1
    else :
      Y_num[y] += 1 


so_li = sorted(Y_num.items() , key = itemgetter(1))
ind = 0
for i in range(10) :
  top_Y[so_li[-i-1][0]] = ind
  ind+=1
  
  
for i in range(21999) :
  y = df['industry'][i]
  s = df['education'][i]
  if(type(s) == str and type(y) == str and y in top_Y) :
    s = s.lower()
    s = s.replace('ug:' , 'undergraduate').replace('pg:' , 'postgraduate ').replace('doctorate:' , 'doctorate ').replace('(',' ').replace(')', ' ').replace('bhm','').replace('b.pharma', 'pharma').replace('b.b.a', 'business').replace('b.m.s','management').replace('gyneocology','gynaecology').replace('obstretrics','obstetrics').replace('opthalmology','ophthalmology').replace('m.pharma', 'pharma').replace('mvsc','Veterinary').replace('b.des.','design').replace('arts&humanities','arts humanities').replace('any specialization','').replace('any graduate','').replace('any postgraduate','')
    k = s.replace(',',' ').replace('-',' ').replace('/' , ' ').split()
    #k = list(set(k))
    if len(k) <= 30 :
      X.append(" ".join(k))
      pred.append(top_Y[y])

      
pred = np.array(pred)

print(len(pred))
print(len(X))
print(pred[0:100])
print(top_Y)

X_ = np.array(X[0:10000])
X_train_indices = sentences_to_indices(X_, wor1d_to_index, 30)
Y_train_oh = convert_to_one_hot(np.array(pred[0:10000]), C = 10)

model.fit(X_train_indices, Y_train_oh, epochs = 10, batch_size = 32, shuffle=True)

from keras.models import load_model
model.save('drive/My Drive/my_model.h5')
model = load_model('drive/My Drive/my_model.h5')

def output(s,model,dictt):
  s = sentences_to_indices(s, word_to_index, 30)
  out = model.predict(s)
  print(out)
  return dictt[np.argmax(out)]

rev_y = dict([(top_Y[k],k) for k in top_Y])
rev_y

import csv
with open('drive/My Drive/dictionary2.csv', 'w') as f:
    for key in rev_y:
        f.write("%s,%s\n"%(key , rev_y[key]))

from keras.models import load_model
model = load_model('drive/My Drive/my_model2.h5')
X_train_indices = sentences_to_indices(np.array(['undergraduate b.tech mechanical']), word_to_index, 10)
predictions = model.predict(X_train_indices)
index = np.argmax(predictions)
print(rev_y[index])

### THe down is the output

### Index value of given input
index